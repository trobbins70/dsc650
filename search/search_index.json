{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About This Course \u00b6 Documentation in Progress Check back soon for more updates. Go to the setup page for instructions on how to setup your computer for this course.","title":"About"},{"location":"#about-this-course","text":"Documentation in Progress Check back soon for more updates. Go to the setup page for instructions on how to setup your computer for this course.","title":"About This Course"},{"location":"data/","text":"Data Sets \u00b6 Berkley DeepDrive \u00b6 Data from Berkley DeepDrive is found in data/external/bdd . License \u00b6 Copyright \u00a92018. The Regents of the University of California (Regents). All Rights Reserved. Permission to use, copy, modify, and distribute this software and its documentation for educational, research, and not-for-profit purposes, without fee and without a signed licensing agreement; and permission use, copy, modify and distribute this software for commercial purposes (such rights not subject to transfer) to BDD member and its affiliates, is hereby granted, provided that the above copyright notice, this paragraph and the following two paragraphs appear in all copies, modifications, and distributions. Contact The Office of Technology Licensing, UC Berkeley, 2150 Shattuck Avenue, Suite 510, Berkeley, CA 94720-1620, (510) 643-7201, otl@berkeley.edu , http://ipira.berkeley.edu/industry-info for commercial licensing opportunities. IN NO EVENT SHALL REGENTS BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF REGENTS HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. REGENTS SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE AND ACCOMPANYING DOCUMENTATION, IF ANY, PROVIDED HEREUNDER IS PROVIDED \"AS IS\". REGENTS HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS. Enron Emails \u00b6 The folder data/external/enron contains a partial copy of the Enron Email dataset . Large Movie Review \u00b6 Large Movie Review Dataset contains 25,000 movie reviews from IMDB and can be found in data/external/imdb . OpenFlights \u00b6 License \u00b6 The OpenFlights Airport, Airline, Plane and Route Databases are made available under the Open Database License . Any rights in individual contents of the database are licensed under the Database Contents License . In short, these mean that you are welcome to use the data as you wish, if and only if you both acknowledge the source and and license any derived works made available to the public with a free license as well. See OpenFlights Data for more detailed documentation. Data \u00b6 OpenFlights data is found in data/external/openflights . Data copied from the OpenFlights Github Repo . Note The special value \\N is used for \\\"NULL\\\" to indicate that no value is available Airports \u00b6 Field Type Nullable? Notes airport_id int No Primary Key name text Yes city text Yes country text Yes iata varchar(3) Yes icao varchar(4) Yes latitude double No longitude double No altitude int Yes timezone float Yes dst char(1) Yes tz_id text Yes type text Yes source text Yes Airlines \u00b6 Field Type Nullable? Notes airline_id int No Primary Key name text No alias text Yes iata varchar(2) Yes icao varchar(3) Yes callsign text Yes country text Yes active boolean No Default value FALSE Routes \u00b6 Field Type Nullable? Notes airline varchar(3) Yes airline_id int Yes src_airport varchar(4) Yes src_airport_id int Yes dst_airport varchar(4) Yes dst_airport_id int Yes codeshare boolean Yes Default value FALSE stops int Yes equipment text Yes airline_id , src_airport_id , and dst_airport_id form a unique key Planes \u00b6 Field Type Nullable? Notes name text Yes iata varchar(3) Yes icao varchar(4) Yes Countries \u00b6 Field Type Nullable? Notes name text Yes iso_code varchar(2) Yes dafif_code varchar(2) Yes Note Some entries have DAFIF codes, but not ISO codes. These are primarily uninhabited islands without airports, and can be ignored for most purposes. Tidynomicon \u00b6 Data copied from the Tidynomicon Github repository . License \u00b6 This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by/4.0/legalcode for the full legal text. This work is licensed under the Creative Commons Attribution 4.0 International license (CC-BY-4.0). You are free to: Share ---copy and redistribute the material in any medium or format Remix ---remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution ---You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions ---You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"Data Sets"},{"location":"data/#data-sets","text":"","title":"Data Sets"},{"location":"data/#berkley-deepdrive","text":"Data from Berkley DeepDrive is found in data/external/bdd .","title":"Berkley DeepDrive"},{"location":"data/#license","text":"Copyright \u00a92018. The Regents of the University of California (Regents). All Rights Reserved. Permission to use, copy, modify, and distribute this software and its documentation for educational, research, and not-for-profit purposes, without fee and without a signed licensing agreement; and permission use, copy, modify and distribute this software for commercial purposes (such rights not subject to transfer) to BDD member and its affiliates, is hereby granted, provided that the above copyright notice, this paragraph and the following two paragraphs appear in all copies, modifications, and distributions. Contact The Office of Technology Licensing, UC Berkeley, 2150 Shattuck Avenue, Suite 510, Berkeley, CA 94720-1620, (510) 643-7201, otl@berkeley.edu , http://ipira.berkeley.edu/industry-info for commercial licensing opportunities. IN NO EVENT SHALL REGENTS BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES, INCLUDING LOST PROFITS, ARISING OUT OF THE USE OF THIS SOFTWARE AND ITS DOCUMENTATION, EVEN IF REGENTS HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. REGENTS SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE SOFTWARE AND ACCOMPANYING DOCUMENTATION, IF ANY, PROVIDED HEREUNDER IS PROVIDED \"AS IS\". REGENTS HAS NO OBLIGATION TO PROVIDE MAINTENANCE, SUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.","title":"License"},{"location":"data/#enron-emails","text":"The folder data/external/enron contains a partial copy of the Enron Email dataset .","title":"Enron Emails"},{"location":"data/#large-movie-review","text":"Large Movie Review Dataset contains 25,000 movie reviews from IMDB and can be found in data/external/imdb .","title":"Large Movie Review"},{"location":"data/#openflights","text":"","title":"OpenFlights"},{"location":"data/#license_1","text":"The OpenFlights Airport, Airline, Plane and Route Databases are made available under the Open Database License . Any rights in individual contents of the database are licensed under the Database Contents License . In short, these mean that you are welcome to use the data as you wish, if and only if you both acknowledge the source and and license any derived works made available to the public with a free license as well. See OpenFlights Data for more detailed documentation.","title":"License"},{"location":"data/#data","text":"OpenFlights data is found in data/external/openflights . Data copied from the OpenFlights Github Repo . Note The special value \\N is used for \\\"NULL\\\" to indicate that no value is available","title":"Data"},{"location":"data/#airports","text":"Field Type Nullable? Notes airport_id int No Primary Key name text Yes city text Yes country text Yes iata varchar(3) Yes icao varchar(4) Yes latitude double No longitude double No altitude int Yes timezone float Yes dst char(1) Yes tz_id text Yes type text Yes source text Yes","title":"Airports"},{"location":"data/#airlines","text":"Field Type Nullable? Notes airline_id int No Primary Key name text No alias text Yes iata varchar(2) Yes icao varchar(3) Yes callsign text Yes country text Yes active boolean No Default value FALSE","title":"Airlines"},{"location":"data/#routes","text":"Field Type Nullable? Notes airline varchar(3) Yes airline_id int Yes src_airport varchar(4) Yes src_airport_id int Yes dst_airport varchar(4) Yes dst_airport_id int Yes codeshare boolean Yes Default value FALSE stops int Yes equipment text Yes airline_id , src_airport_id , and dst_airport_id form a unique key","title":"Routes"},{"location":"data/#planes","text":"Field Type Nullable? Notes name text Yes iata varchar(3) Yes icao varchar(4) Yes","title":"Planes"},{"location":"data/#countries","text":"Field Type Nullable? Notes name text Yes iso_code varchar(2) Yes dafif_code varchar(2) Yes Note Some entries have DAFIF codes, but not ISO codes. These are primarily uninhabited islands without airports, and can be ignored for most purposes.","title":"Countries"},{"location":"data/#tidynomicon","text":"Data copied from the Tidynomicon Github repository .","title":"Tidynomicon"},{"location":"data/#license_2","text":"This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by/4.0/legalcode for the full legal text. This work is licensed under the Creative Commons Attribution 4.0 International license (CC-BY-4.0). You are free to: Share ---copy and redistribute the material in any medium or format Remix ---remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution ---You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions ---You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","title":"License"},{"location":"lessons/","text":"Overview \u00b6","title":"Lessons"},{"location":"lessons/#overview","text":"","title":"Overview"},{"location":"lessons/fundamentals/","text":"Introduction \u00b6 Documentation in Progress Check back soon for more updates.","title":"Introduction"},{"location":"lessons/fundamentals/#introduction","text":"Documentation in Progress Check back soon for more updates.","title":"Introduction"},{"location":"lessons/fundamentals/latency/","text":"Latency \u00b6 Latency Numbers Every Data Scientist Should Know In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know . These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon . Below is yet another update on these numbers with data taken from Colin Scott , a Berkeley researcher. An interactive version of this repository can be found here . Action Latency (ns) 1 Latency (\u03bc) 2 Latency (ms) 3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA->Netherlands->CA 150,000,000 ns 150,000 \u03bcs 150 ms Notes 1 ns = 10 -9 seconds \u21a9 1 \u03bcs = 10 -6 seconds = 1,000 ns \u21a9 1 ms = 10 -3 seconds = 1,000 \u03bcs = 1,000,000 ns \u21a9","title":"Latency Numbers"},{"location":"lessons/fundamentals/latency/#latency","text":"Latency Numbers Every Data Scientist Should Know In the 2000s, Jeff Dean, a Google Senior Fellow in their Research Group, presented a list of latency numbers that every programmer should know . These numbers describe how long it takes to perform certain actions within distributed programs. Since then, it has been updated and expanded upon . Below is yet another update on these numbers with data taken from Colin Scott , a Berkeley researcher. An interactive version of this repository can be found here . Action Latency (ns) 1 Latency (\u03bc) 2 Latency (ms) 3 L1 cache reference 1 ns Branch mispredict 3 ns L2 cache reference 4 ns Mutex lock/unlock 17 ns Main memory reference 100 ns Compress 1KB with Zippy 2,000 ns 2 \u03bcs Send 1KB over 1 Gbps network 10,000 ns 10 \u03bcs SSD random read 16,000 ns 16 \u03bcs Read 1 MB sequentially from SSD 49,000 ns 49 \u03bcs Read 1 MB sequentially from memory 250,000 ns 250 \u03bcs Round trip within same datacenter 500,000 ns 500 \u03bcs Read 1 MB sequentially from disk 825,000 ns 825 \u03bcs Disk seek 2,000,000 ns 2,000 \u03bcs 2 ms Send packet CA->Netherlands->CA 150,000,000 ns 150,000 \u03bcs 150 ms Notes 1 ns = 10 -9 seconds \u21a9 1 \u03bcs = 10 -6 seconds = 1,000 ns \u21a9 1 ms = 10 -3 seconds = 1,000 \u03bcs = 1,000,000 ns \u21a9","title":"Latency"},{"location":"lessons/fundamentals/size/","text":"Data Size \u00b6 Metric Value Unit Symbol Notes 1 1 Byte B 1 byte = 1 letter in computer memory 10^{3} 10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6} 10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9} 10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12} 10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15} 10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18} 10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21} 10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24} 10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth Binary Value Unit Symbol Notes 2^{10} 2^{10} Kibibyte Ki 1024 bytes 2^{20} 2^{20} Mebibyte Mi 1024 kibibytes 2^{30} 2^{30} Gibibyte Gi 1024 mebibytes 2^{40} 2^{40} Tebibyte Ti 1024 gibibytes 2^{50} 2^{50} Pebibyte Pi 1024 tebibytes 2^{60} 2^{60} Exbibyte Ei 1024 pebibytes 2^{70} 2^{70} Zebibyte Zi 1024 exbibytes 2^{80} 2^{80} Yobibyte Yi 1024 zebibytes","title":"Data Size"},{"location":"lessons/fundamentals/size/#data-size","text":"Metric Value Unit Symbol Notes 1 1 Byte B 1 byte = 1 letter in computer memory 10^{3} 10^{3} Kilobyte kB 2 kB = RAM on original NES 10^{6} 10^{6} Megabyte MB 1 MB \u2248 1 HD quality photo 10^{9} 10^{9} Gigabyte GB 1 GB \u2248 114 minutes of uncompressed CD audio 10^{12} 10^{12} Terabyte TB 1.9 TB \u2248 Size of all multimedia files used in English wikipedia on May 2012 10^{15} 10^{15} Petabyte PB 10 PB \u2248 Size of Library of Congress collection in 2005 10^{18} 10^{18} Exabyte EB 15 EB \u2248 storage space at Google data warehouse as of 2013 10^{21} 10^{21} Zettabyte ZB 6.9 ZB \u2248 amount of data accessed by Americans in 2012 10^{24} 10^{24} Yottabyte YB 1 YB \u2248 131 TB for every person on Earth Binary Value Unit Symbol Notes 2^{10} 2^{10} Kibibyte Ki 1024 bytes 2^{20} 2^{20} Mebibyte Mi 1024 kibibytes 2^{30} 2^{30} Gibibyte Gi 1024 mebibytes 2^{40} 2^{40} Tebibyte Ti 1024 gibibytes 2^{50} 2^{50} Pebibyte Pi 1024 tebibytes 2^{60} 2^{60} Exbibyte Ei 1024 pebibytes 2^{70} 2^{70} Zebibyte Zi 1024 exbibytes 2^{80} 2^{80} Yobibyte Yi 1024 zebibytes","title":"Data Size"},{"location":"setup/","text":"Overview \u00b6 Documentation in Progress Check back soon for more updates. Operating System Dependencies \u00b6 See the pages macOS , Ubuntu , or Windows 10 for how to install the operating system specific dependencies for your computer. Clone Github Repository \u00b6 Start by cloning or downloading this repository to your local computer. You can clone this repository using the Github Desktop Client or using the Git command line. Clone using SSH git clone git@github.com:bellevue-university/dsc650.git Clone using HTTPS git clone https://github.com/bellevue-university/dsc650.git You will need access to this repository throughout the course, so place it in a reliable location. Python Dependencies \u00b6 Note Some of the dependencies do not work with Python 3.8, so you will need to install Python 3.7. Poetry \u00b6 Poetry is a dependency management and packaging tool for Python. You can use it to declare libraries for your project and manage (install/update) them for you. macOS, Linux, or Bash on Windows curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python Windows PowerShell ( Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing ) .Content | python For this project you can use poetry install to install the project dependencies. This command will create a virtual environment for your project and install the dependencies. You can use the poetry shell command to activate this virtual environment. Within the root directory of the dsc650 repository, run the following commands to setup your Python environment using Poetry. $ poetry env use 3.7 Creating virtualenv dsc650-Q1YxCGsa-py3.7 in /usr/local/var/virtualenvs Using virtualenv: /usr/local/var/virtualenvs/dsc650-Q1YxCGsa-py3.7 $ poetry install Installing dependencies from lock file Package operations: 143 installs, 0 updates, 0 removals - Installing decorator (4.4.2) - Installing ipython-genutils (0.2.0) - Installing six (1.14.0) - Installing zipp (3.1.0) - Installing attrs (19.3.0) . . . - Installing tensorflow (2.1.0) - Installing tinydb (4.1.1) - Installing zodb (5.5.1) - Installing dsc650 (0.1.0) Once completed, you can use poetry shell to activate the virtual environment. $ poetry shell Spawning shell within /usr/local/var/virtualenvs/dsc650-Q1YxCGsa-py3.7 You can add this interpreter to an existing PyCharm project by selecting the PyCharm menu and then select Preferences -> Project -> Project Interpreter . In this example, you would add /usr/local/var/virtualenvs/dsc650-Q1YxCGsa-py3.7/bin/python as the interpreter. See Poetry for additional documentation on basic usage. Virtualenv and Pip \u00b6 You can use virtualenv and Pip if you don't want to use Poetry. Simply use the following commands. pip install virtualenv virtualenv venv source venv/bin/activate pip install -r requirements.txt You can add this interpreter to an existing PyCharm project by selecting the PyCharm menu and then select Preferences -> Project -> Project Interpreter . In this example, you would add venv/bin/python as the interpreter. PyCharm \u00b6 After you are finished cloning or downloading the repository, you will open the repository directory using PyCharm. You can do this by opening PyCharm and selecting the File -> Open option and choosing the directory containing the repository. Download PyCharm if you do not already have it installed on your computer. You can also download PyCharm for Anaconda to install PyCharm with the Anaconda Python interpreter.","title":"Overview"},{"location":"setup/#overview","text":"Documentation in Progress Check back soon for more updates.","title":"Overview"},{"location":"setup/#operating-system-dependencies","text":"See the pages macOS , Ubuntu , or Windows 10 for how to install the operating system specific dependencies for your computer.","title":"Operating System Dependencies"},{"location":"setup/#clone-github-repository","text":"Start by cloning or downloading this repository to your local computer. You can clone this repository using the Github Desktop Client or using the Git command line. Clone using SSH git clone git@github.com:bellevue-university/dsc650.git Clone using HTTPS git clone https://github.com/bellevue-university/dsc650.git You will need access to this repository throughout the course, so place it in a reliable location.","title":"Clone Github Repository"},{"location":"setup/#python-dependencies","text":"Note Some of the dependencies do not work with Python 3.8, so you will need to install Python 3.7.","title":"Python Dependencies"},{"location":"setup/#poetry","text":"Poetry is a dependency management and packaging tool for Python. You can use it to declare libraries for your project and manage (install/update) them for you. macOS, Linux, or Bash on Windows curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python Windows PowerShell ( Invoke-WebRequest -Uri https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py -UseBasicParsing ) .Content | python For this project you can use poetry install to install the project dependencies. This command will create a virtual environment for your project and install the dependencies. You can use the poetry shell command to activate this virtual environment. Within the root directory of the dsc650 repository, run the following commands to setup your Python environment using Poetry. $ poetry env use 3.7 Creating virtualenv dsc650-Q1YxCGsa-py3.7 in /usr/local/var/virtualenvs Using virtualenv: /usr/local/var/virtualenvs/dsc650-Q1YxCGsa-py3.7 $ poetry install Installing dependencies from lock file Package operations: 143 installs, 0 updates, 0 removals - Installing decorator (4.4.2) - Installing ipython-genutils (0.2.0) - Installing six (1.14.0) - Installing zipp (3.1.0) - Installing attrs (19.3.0) . . . - Installing tensorflow (2.1.0) - Installing tinydb (4.1.1) - Installing zodb (5.5.1) - Installing dsc650 (0.1.0) Once completed, you can use poetry shell to activate the virtual environment. $ poetry shell Spawning shell within /usr/local/var/virtualenvs/dsc650-Q1YxCGsa-py3.7 You can add this interpreter to an existing PyCharm project by selecting the PyCharm menu and then select Preferences -> Project -> Project Interpreter . In this example, you would add /usr/local/var/virtualenvs/dsc650-Q1YxCGsa-py3.7/bin/python as the interpreter. See Poetry for additional documentation on basic usage.","title":"Poetry"},{"location":"setup/#virtualenv-and-pip","text":"You can use virtualenv and Pip if you don't want to use Poetry. Simply use the following commands. pip install virtualenv virtualenv venv source venv/bin/activate pip install -r requirements.txt You can add this interpreter to an existing PyCharm project by selecting the PyCharm menu and then select Preferences -> Project -> Project Interpreter . In this example, you would add venv/bin/python as the interpreter.","title":"Virtualenv and Pip"},{"location":"setup/#pycharm","text":"After you are finished cloning or downloading the repository, you will open the repository directory using PyCharm. You can do this by opening PyCharm and selecting the File -> Open option and choosing the directory containing the repository. Download PyCharm if you do not already have it installed on your computer. You can also download PyCharm for Anaconda to install PyCharm with the Anaconda Python interpreter.","title":"PyCharm"},{"location":"setup/macOS/","text":"macOS \u00b6 Documentation in Progress Check back soon for more updates. Package Manager \u00b6 If you are using macOS as your primary development environment, I recommend using a package manager like homebrew . A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use aptitude . Red Hat and Fedora systems use yum . MacPorts and homebrew are two popular package managers for macOS. Chocolatey is a popular package manager for Windows. You can install Homebrew on your system by executing the following command in your terminal. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" I also recommend using Homebrew Cask to install graphical applications like Atom and Google Chrome. After you have installed Homebrew, update your package index. brew update Finally, install the following packages required for this course. brew install apache-arrow brew install apache-spark brew install avro-tools brew install git brew install hadoop brew install libtensorflow brew install pandoc brew install pandoc-citeproc brew install pandoc-crossref brew install parquet-tools brew install protobuf brew install snappy Optionally, you can install the following packages using Homebrew Cask. brew cask install anaconda brew cask install atom brew cask install github brew cask install mactex brew cask install miniconda brew cask install virtualbox JDK \u00b6 Spark and Hadoop use version 8 of the Java Development Kit (JDK 8). Download the latest version from Oracle and install on your local machine. Once completed, edit your shell profile \u2013 $HOME/.bash_profile if you are using Bash or $HOME/.zshrc if you are using Zsh \u2013 and add the following. export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home TensorFlow \u00b6 Note There is no GPU support for macOS","title":"macOS"},{"location":"setup/macOS/#macos","text":"Documentation in Progress Check back soon for more updates.","title":"macOS"},{"location":"setup/macOS/#package-manager","text":"If you are using macOS as your primary development environment, I recommend using a package manager like homebrew . A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use aptitude . Red Hat and Fedora systems use yum . MacPorts and homebrew are two popular package managers for macOS. Chocolatey is a popular package manager for Windows. You can install Homebrew on your system by executing the following command in your terminal. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" I also recommend using Homebrew Cask to install graphical applications like Atom and Google Chrome. After you have installed Homebrew, update your package index. brew update Finally, install the following packages required for this course. brew install apache-arrow brew install apache-spark brew install avro-tools brew install git brew install hadoop brew install libtensorflow brew install pandoc brew install pandoc-citeproc brew install pandoc-crossref brew install parquet-tools brew install protobuf brew install snappy Optionally, you can install the following packages using Homebrew Cask. brew cask install anaconda brew cask install atom brew cask install github brew cask install mactex brew cask install miniconda brew cask install virtualbox","title":"Package Manager"},{"location":"setup/macOS/#jdk","text":"Spark and Hadoop use version 8 of the Java Development Kit (JDK 8). Download the latest version from Oracle and install on your local machine. Once completed, edit your shell profile \u2013 $HOME/.bash_profile if you are using Bash or $HOME/.zshrc if you are using Zsh \u2013 and add the following. export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_211.jdk/Contents/Home","title":"JDK"},{"location":"setup/macOS/#tensorflow","text":"Note There is no GPU support for macOS","title":"TensorFlow"},{"location":"setup/ubuntu/","text":"Ubuntu \u00b6 Documentation in Progress Check back soon for more updates. System \u00b6 Edit /etc/hosts : 127 .0.0.1 hostname Install JDK, Scala and Git : sudo apt install default-jdk scala git -y Install Poetry : curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3 Install Oracle JDK : sudo apt update sudo add-apt-repository ppa:webupd8team/java sudo apt update sudo apt install oracle-java8-installer oracle-java8-set-default Spark \u00b6 This guide provides more information on how to setup Spark on Ubuntu. Start by downloading Spark 2.4.5 for Hadoop 2.7. curl -O https://www.apache.org/dyn/closer.lua/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz Extract the archive. tar xvf spark-2.4.5-bin-hadoop2.7.tgz Move it to /opt/spark . sudo mv spark-2.4.5-bin-hadoop2.7/ /opt/spark Update the environment variables by adding the following to your shell profile. export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin export PYSPARK_PYTHON = /usr/bin/python3 Alternatively, add it to your profile using echo . echo \"export SPARK_HOME=/opt/spark\" >> ~/.profile echo \"export PATH= $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin\" >> ~/.profile echo \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.profile This assumes your profile is in .profile . It may also be in another location like ~/.bashrc or ~/.zshrc . Activate your changes as follows. source ~/.bashrc Start a stand-alone server. start-master.sh The process will listen on 8080. ss -tunelp | grep 8080 tcp LISTEN 0 1 *:8080 Start a worker process. start-slave.sh spark://ubuntu:7077 You can stop the processes using the following commands. stop-slave.sh stop-master.sh TensorFlow \u00b6 Ubuntu 18.04 ships with Python 3 by default sudo apt install python3-venv Note If you have a dedicated NVIDIA GPU and want to take advantage of its processing power, instead of tensorflow install the tensorflow-gpu package which includes GPU support. GPU Support \u00b6 Check the following links to more information on GPU support. CUDA GPUs TensorFlow GPU Install # Add NVIDIA package repositories wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb sudo apt-get update wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt-get update # Install NVIDIA driver sudo apt-get install --no-install-recommends nvidia-driver-430 # Reboot. Check that GPUs are visible using the command: nvidia-smi # Install development and runtime libraries (~4GB) sudo apt-get install --no-install-recommends \\ cuda-10-1 \\ libcudnn7 = 7 .6.4.38-1+cuda10.1 \\ libcudnn7-dev = 7 .6.4.38-1+cuda10.1 # Install TensorRT. Requires that libcudnn7 is installed above. sudo apt-get install -y --no-install-recommends libnvinfer6 = 6 .0.1-1+cuda10.1 \\ libnvinfer-dev = 6 .0.1-1+cuda10.1 \\ libnvinfer-plugin6 = 6 .0.1-1+cuda10.1","title":"Ubuntu"},{"location":"setup/ubuntu/#ubuntu","text":"Documentation in Progress Check back soon for more updates.","title":"Ubuntu"},{"location":"setup/ubuntu/#system","text":"Edit /etc/hosts : 127 .0.0.1 hostname Install JDK, Scala and Git : sudo apt install default-jdk scala git -y Install Poetry : curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python3 Install Oracle JDK : sudo apt update sudo add-apt-repository ppa:webupd8team/java sudo apt update sudo apt install oracle-java8-installer oracle-java8-set-default","title":"System"},{"location":"setup/ubuntu/#spark","text":"This guide provides more information on how to setup Spark on Ubuntu. Start by downloading Spark 2.4.5 for Hadoop 2.7. curl -O https://www.apache.org/dyn/closer.lua/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz Extract the archive. tar xvf spark-2.4.5-bin-hadoop2.7.tgz Move it to /opt/spark . sudo mv spark-2.4.5-bin-hadoop2.7/ /opt/spark Update the environment variables by adding the following to your shell profile. export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin export PYSPARK_PYTHON = /usr/bin/python3 Alternatively, add it to your profile using echo . echo \"export SPARK_HOME=/opt/spark\" >> ~/.profile echo \"export PATH= $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin\" >> ~/.profile echo \"export PYSPARK_PYTHON=/usr/bin/python3\" >> ~/.profile This assumes your profile is in .profile . It may also be in another location like ~/.bashrc or ~/.zshrc . Activate your changes as follows. source ~/.bashrc Start a stand-alone server. start-master.sh The process will listen on 8080. ss -tunelp | grep 8080 tcp LISTEN 0 1 *:8080 Start a worker process. start-slave.sh spark://ubuntu:7077 You can stop the processes using the following commands. stop-slave.sh stop-master.sh","title":"Spark"},{"location":"setup/ubuntu/#tensorflow","text":"Ubuntu 18.04 ships with Python 3 by default sudo apt install python3-venv Note If you have a dedicated NVIDIA GPU and want to take advantage of its processing power, instead of tensorflow install the tensorflow-gpu package which includes GPU support.","title":"TensorFlow"},{"location":"setup/ubuntu/#gpu-support","text":"Check the following links to more information on GPU support. CUDA GPUs TensorFlow GPU Install # Add NVIDIA package repositories wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb sudo apt-get update wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb sudo apt-get update # Install NVIDIA driver sudo apt-get install --no-install-recommends nvidia-driver-430 # Reboot. Check that GPUs are visible using the command: nvidia-smi # Install development and runtime libraries (~4GB) sudo apt-get install --no-install-recommends \\ cuda-10-1 \\ libcudnn7 = 7 .6.4.38-1+cuda10.1 \\ libcudnn7-dev = 7 .6.4.38-1+cuda10.1 # Install TensorRT. Requires that libcudnn7 is installed above. sudo apt-get install -y --no-install-recommends libnvinfer6 = 6 .0.1-1+cuda10.1 \\ libnvinfer-dev = 6 .0.1-1+cuda10.1 \\ libnvinfer-plugin6 = 6 .0.1-1+cuda10.1","title":"GPU Support"},{"location":"setup/windows/","text":"Windows \u00b6 Documentation in Progress Check back soon for more updates. Overview \u00b6 There are multiple options available for installing Spark, Hadoop, TensorFlow, and other Big Data/Deep Learning software on Windows 10. While it is possible to install these packages and use these packages on Windows, I strongly urge you to heed the warning of Fran\u00e7ois Chollet, author of Deep Learning With Python . Whether you\u2019re running locally or in the cloud, it\u2019s better to be using a Unix workstation. Although it\u2019s technically possible to use Keras on Windows (all three Keras backends support Windows), We don\u2019t recommend it. In the installation instructions in appendix A, we\u2019ll consider an Ubuntu machine. If you\u2019re a Windows user, the simplest solution to get everything running is to set up an Ubuntu dual boot on your machine. It may seem like a hassle, but using Ubuntu will save you a lot of time and trouble in the long run. Package Manager \u00b6 If you are using Windows as your primary development environment, I recommend using a package manager like Chocolatey . A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use aptitude . Red Hat and Fedora systems use yum . MacPorts and homebrew are two popular package managers for macOS. Follow the Chocolatey installation guide to install the package manager on your system. Once you have completed installing the package manager, you can install new software by running PowerShell as an administrator and using the choco command. For example, the following commands will install the latest versions of Adobe Acrobat Reader, Google Chrome, and FireFox on your system. choco install adobereader choco install googlechrome choco install firefox You can upgrade all packages using choco upgrade all or upgrade individual packages using choco upgrade firefox . Similarly, you can uninstall packages using choco uninstall . The following is a table of software you might find useful for this course. Software Package Name Anaconda Distribution (Python 3.x) anaconda3 Git (Install) git.install GitHub Desktop github-desktop Graphviz graphviz Hadoop hadoop Java Development Kit 8 jdk8 JetBrains Toolbox App jetbrainstoolbox JetBrains DataGrip datagrip JetBrains PyCharm pycharm JetBrains PyCharm Educational pycharm-edu JetBrains PyCharm (Community Edition) 1 pycharm-community MikTeX miktex Pandoc pandoc Pandoc CrossRef pandoc-crossref PostgreSQL postgresql Protocol Buffers protoc Scala scala VirtualBox virtualbox If you are interested to see what other packages are available, see Chocolatey packages for a list of community maintained packages. Installing Dependencies \u00b6 Option 1: Dual Booting Ubuntu \u00b6 Documentation in Progress Check back soon for more updates. Download latest Ubuntu ISO Ubuntu 18.04 Ubuntu 20.04 Ubuntu Flavors Lubuntu Xubuntu Ubuntu Budgie Ubuntu Mate Option 2: Run Ubuntu in a Virtual Machine \u00b6 Documentation in Progress Check back soon for more updates. Run on VirtualBox. Option 3: Windows Linux Sub-System \u00b6 You can install Ubuntu on Windows 10 using the Windows Subsystem for Linux. See the installation guide for more information. Option 4: Native Packages \u00b6 Warning This option is prone to error and the instructor may not be able to help you troubleshoot your development environment. Use at your own risk. The following article provides instructions on how to install PySpark on Windows 10 . Install Hadoop using Chocolatey Install JDK 8 via Chocolatey Download Winutils Copy 2.7.1 bin in winutils to hadoop-3.2.1/bin Download Spark (2.4.5 release, pre-built for Hadoop 2.7) at http://spark.apache.org/downloads.html Copy the decompressed Spark folders to C:\\Spark Set the following environment variables Variable Value HADOOP_HOME C:\\Hadoop\\hadoop-3.2.1 JAVA_HOME C:\\Program Files\\Java\\jdk1.8.0_211 SPARK_HOME C:\\Spark Setting Environment Variables \u00b6 Go the edit system environment variables in your control panel. Under System Properties -> Advanced select Environment Variables . Change the environment variables for your user. While you can use the community version of PyCharm, JetBrains offers free educational licenses for students and teachers. See educational licenses for more details. \u21a9","title":"Windows"},{"location":"setup/windows/#windows","text":"Documentation in Progress Check back soon for more updates.","title":"Windows"},{"location":"setup/windows/#overview","text":"There are multiple options available for installing Spark, Hadoop, TensorFlow, and other Big Data/Deep Learning software on Windows 10. While it is possible to install these packages and use these packages on Windows, I strongly urge you to heed the warning of Fran\u00e7ois Chollet, author of Deep Learning With Python . Whether you\u2019re running locally or in the cloud, it\u2019s better to be using a Unix workstation. Although it\u2019s technically possible to use Keras on Windows (all three Keras backends support Windows), We don\u2019t recommend it. In the installation instructions in appendix A, we\u2019ll consider an Ubuntu machine. If you\u2019re a Windows user, the simplest solution to get everything running is to set up an Ubuntu dual boot on your machine. It may seem like a hassle, but using Ubuntu will save you a lot of time and trouble in the long run.","title":"Overview"},{"location":"setup/windows/#package-manager","text":"If you are using Windows as your primary development environment, I recommend using a package manager like Chocolatey . A package manager is a tool that automates the process of installing, updating, configuring, and removing computer programs. Package managers are commonly used on Unix and Linux distributions. Debian Linux systems, like Ubuntu, use aptitude . Red Hat and Fedora systems use yum . MacPorts and homebrew are two popular package managers for macOS. Follow the Chocolatey installation guide to install the package manager on your system. Once you have completed installing the package manager, you can install new software by running PowerShell as an administrator and using the choco command. For example, the following commands will install the latest versions of Adobe Acrobat Reader, Google Chrome, and FireFox on your system. choco install adobereader choco install googlechrome choco install firefox You can upgrade all packages using choco upgrade all or upgrade individual packages using choco upgrade firefox . Similarly, you can uninstall packages using choco uninstall . The following is a table of software you might find useful for this course. Software Package Name Anaconda Distribution (Python 3.x) anaconda3 Git (Install) git.install GitHub Desktop github-desktop Graphviz graphviz Hadoop hadoop Java Development Kit 8 jdk8 JetBrains Toolbox App jetbrainstoolbox JetBrains DataGrip datagrip JetBrains PyCharm pycharm JetBrains PyCharm Educational pycharm-edu JetBrains PyCharm (Community Edition) 1 pycharm-community MikTeX miktex Pandoc pandoc Pandoc CrossRef pandoc-crossref PostgreSQL postgresql Protocol Buffers protoc Scala scala VirtualBox virtualbox If you are interested to see what other packages are available, see Chocolatey packages for a list of community maintained packages.","title":"Package Manager"},{"location":"setup/windows/#installing-dependencies","text":"","title":"Installing Dependencies"},{"location":"setup/windows/#option-1-dual-booting-ubuntu","text":"Documentation in Progress Check back soon for more updates. Download latest Ubuntu ISO Ubuntu 18.04 Ubuntu 20.04 Ubuntu Flavors Lubuntu Xubuntu Ubuntu Budgie Ubuntu Mate","title":"Option 1: Dual Booting Ubuntu"},{"location":"setup/windows/#option-2-run-ubuntu-in-a-virtual-machine","text":"Documentation in Progress Check back soon for more updates. Run on VirtualBox.","title":"Option 2: Run Ubuntu in a Virtual Machine"},{"location":"setup/windows/#option-3-windows-linux-sub-system","text":"You can install Ubuntu on Windows 10 using the Windows Subsystem for Linux. See the installation guide for more information.","title":"Option 3: Windows Linux Sub-System"},{"location":"setup/windows/#option-4-native-packages","text":"Warning This option is prone to error and the instructor may not be able to help you troubleshoot your development environment. Use at your own risk. The following article provides instructions on how to install PySpark on Windows 10 . Install Hadoop using Chocolatey Install JDK 8 via Chocolatey Download Winutils Copy 2.7.1 bin in winutils to hadoop-3.2.1/bin Download Spark (2.4.5 release, pre-built for Hadoop 2.7) at http://spark.apache.org/downloads.html Copy the decompressed Spark folders to C:\\Spark Set the following environment variables Variable Value HADOOP_HOME C:\\Hadoop\\hadoop-3.2.1 JAVA_HOME C:\\Program Files\\Java\\jdk1.8.0_211 SPARK_HOME C:\\Spark","title":"Option 4: Native Packages"},{"location":"setup/windows/#setting-environment-variables","text":"Go the edit system environment variables in your control panel. Under System Properties -> Advanced select Environment Variables . Change the environment variables for your user. While you can use the community version of PyCharm, JetBrains offers free educational licenses for students and teachers. See educational licenses for more details. \u21a9","title":"Setting Environment Variables"}]}